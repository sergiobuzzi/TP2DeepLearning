{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "TrabajoPractico2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIMnCIO3MV0K",
        "colab_type": "text"
      },
      "source": [
        "# Aprendizaje Profundo - Práctico 2\n",
        "\n",
        "Integrantes:\n",
        "\n",
        "\n",
        "*   Buzzi, Sergio\n",
        "*   Diaz, Carolina\n",
        "*   Fabro, Juan\n",
        "\n",
        "En este notebook, se desarolla el práctico 2. El Objetivo principal de dicho práctico es incorporal un embedding de palabras para aprovechar la variable 'DESCRIPTION'.\n",
        "Se efectuaron varios experimentos, en los cuales se exploraron distintas configuraciones arquitectónicas de la red final y en particular del dropout, dado que cuando se sube el modelo a la competencia de kaggle el ajuste decae drásticamente, lo que estaría indicando overfitting en train y development.\n",
        "\n",
        "Como ya conocemos el dataset porque el mismo fue utilizado en una materia previa, no esperamos una accuracy superior al 40 % en el conjunto de test.\n",
        "\n",
        "A continuación se presenta un modelo en detalle y luego se informa brevemente en otro documento cuales fueron los resultados de los diversos modelos experimentados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0x95qbYMomu",
        "colab_type": "code",
        "outputId": "06cc8210-0c98-44f3-a0c8-99626836d37a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%bash\n",
        "pip install --upgrade pip\n",
        "pip install --upgrade tensorflow-gpu==2.0.0\n",
        "pip install --upgrade mlflow graphviz pydot"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (19.3.1)\n",
            "Requirement already up-to-date: tensorflow-gpu==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.8.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (42.0.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.2.7)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Requirement already up-to-date: mlflow in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already up-to-date: graphviz in /usr/local/lib/python3.6/dist-packages (0.13.2)\n",
            "Requirement already up-to-date: pydot in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: querystring-parser in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.2.4)\n",
            "Requirement already satisfied, skipping upgrade: databricks-cli>=0.8.7 in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.9.1)\n",
            "Requirement already satisfied, skipping upgrade: docker>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (4.1.0)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: gitpython>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.25.3)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.17.3 in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.3.11)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied, skipping upgrade: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: sqlparse in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3.0)\n",
            "Requirement already satisfied, skipping upgrade: alembic in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: simplejson in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.17.0)\n",
            "Requirement already satisfied, skipping upgrade: gunicorn; platform_system != \"Windows\" in /usr/local/lib/python3.6/dist-packages (from mlflow) (20.0.4)\n",
            "Requirement already satisfied, skipping upgrade: gorilla in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3.0)\n",
            "Requirement already satisfied, skipping upgrade: Flask in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.4.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->mlflow) (42.0.2)\n",
            "Requirement already satisfied, skipping upgrade: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.6)\n",
            "Requirement already satisfied, skipping upgrade: configparser>=0.3.5 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.7->mlflow) (4.0.2)\n",
            "Requirement already satisfied, skipping upgrade: websocket-client>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from docker>=4.0.0->mlflow) (0.56.0)\n",
            "Requirement already satisfied, skipping upgrade: gitdb2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from gitpython>=2.1.0->mlflow) (2.0.6)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mlflow) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: Mako in /usr/local/lib/python3.6/dist-packages (from alembic->mlflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic->mlflow) (1.0.4)\n",
            "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (2.10.3)\n",
            "Requirement already satisfied, skipping upgrade: smmap2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from gitdb2>=2.0.0->gitpython>=2.1.0->mlflow) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->mlflow) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "XpJDQPN7MV0r",
        "colab_type": "code",
        "outputId": "b0ae5e28-dd94-4fe4-878e-0e37d3f8d53d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import SVG\n",
        "from gensim import corpora\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from pprint import pprint\n",
        "\n",
        "nltk.download([\"punkt\", \"stopwords\"]);"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fHSkXnpMmEr",
        "colab_type": "code",
        "outputId": "6523f68e-209e-4ffd-9243-6560d842f783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5cPDPatMV02",
        "colab_type": "text"
      },
      "source": [
        "## Carga de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3gKbFA2MV03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIRECTORY = '/content/drive/My Drive/TP2_deeplearning/petfinder_dataset/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AlKdefQMV08",
        "colab_type": "code",
        "outputId": "f2f25c93-2c6d-4f77-e8ae-6ec5a5af4756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "dataset = pd.read_csv(os.path.join(DATA_DIRECTORY, 'train.csv'))\n",
        "\n",
        "target_col = 'AdoptionSpeed'\n",
        "nlabels = dataset[target_col].unique().shape[0]\n",
        "\n",
        "dataset.head(3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Age</th>\n",
              "      <th>Breed1</th>\n",
              "      <th>Breed2</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Color1</th>\n",
              "      <th>Color2</th>\n",
              "      <th>Color3</th>\n",
              "      <th>MaturitySize</th>\n",
              "      <th>FurLength</th>\n",
              "      <th>Vaccinated</th>\n",
              "      <th>Dewormed</th>\n",
              "      <th>Sterilized</th>\n",
              "      <th>Health</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Fee</th>\n",
              "      <th>State</th>\n",
              "      <th>Description</th>\n",
              "      <th>AdoptionSpeed</th>\n",
              "      <th>PID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>299</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>41326</td>\n",
              "      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>307</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>150</td>\n",
              "      <td>41401</td>\n",
              "      <td>Good guard dog, very alert, active, obedience ...</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>307</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>41326</td>\n",
              "      <td>This handsome yet cute boy is up for adoption....</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Type  Age  ...  AdoptionSpeed  PID\n",
              "0     2    3  ...              2    0\n",
              "1     1    4  ...              2    3\n",
              "2     1    1  ...              2    4\n",
              "\n",
              "[3 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eWkUNC_MV1B",
        "colab_type": "text"
      },
      "source": [
        "## Preproceso del texto para agregarlo como feature\n",
        "\n",
        "### Tokenización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LT2qJRqMV1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SW = set(stopwords.words(\"english\"))\n",
        "\n",
        "def tokenize_description(description):\n",
        "    return [w.lower() for w in word_tokenize(description, language=\"english\") if w.lower() not in SW]\n",
        "\n",
        "# Fill the null values with the empty string to avoid errors with NLTK tokenization\n",
        "dataset[\"TokenizedDescription\"] = dataset[\"Description\"].fillna(value=\"\").apply(tokenize_description)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnmxNW8QMV1H",
        "colab_type": "text"
      },
      "source": [
        "### Detección de tamaño de las descripciones\n",
        "\n",
        "Un punto importante a tener en cuenta es que las descripciones tienen tamaño variable, y esto no es compatible con los algoritmos de aprendizaje automático. Por lo que hay que llevar las secuencias a un tamaño uniforme.\n",
        "\n",
        "Para definir dicho tamaño uniforme, es útil mirar qué tamaños mínimos, máximos y medios manejan las descripciones y a partir de esto establecer el tamaño máximo de la secuencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZpayjKPMV1J",
        "colab_type": "code",
        "outputId": "80943306-a62d-4412-a85e-c7fc9cf53a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "pprint(dataset[\"TokenizedDescription\"].apply(len).describe())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    10582.000000\n",
            "mean        44.418541\n",
            "std         48.464623\n",
            "min          0.000000\n",
            "25%         16.000000\n",
            "50%         31.000000\n",
            "75%         55.000000\n",
            "max        803.000000\n",
            "Name: TokenizedDescription, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8YLAtqBMV1O",
        "colab_type": "text"
      },
      "source": [
        "Vemos que más del 75% de las secuencias tienen 55 palabras o menos. Esto es un buen punto de partida, así que podemos establecer el tamaño máximo de las secuencia en 55 palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGxzTofLMV1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LEN = 55"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QkbK4XjMV1U",
        "colab_type": "text"
      },
      "source": [
        "### Vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l5U2-zjMV1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary = corpora.Dictionary(dataset[\"TokenizedDescription\"])\n",
        "vocabulary.filter_extremes(no_below=1, no_above=1.0, keep_n=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGGphagjMV1a",
        "colab_type": "text"
      },
      "source": [
        "### Word Embeddings (GloVe)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvBTojrYMV1b",
        "colab_type": "code",
        "outputId": "219b49f0-b6b8-4de4-cb22-9ac612ecb429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embeddings_index = {}\n",
        "\n",
        "with open(\"/content/drive/My Drive/TP2_deeplearning/petfinder_dataset/glove.6B.100d.txt\", \"r\", encoding=\"utf8\") as fh:\n",
        "    for line in fh:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        if word in vocabulary.token2id:  # Only use the embeddings of words in our vocabulary\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found {} word vectors.\".format(len(embeddings_index)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7897 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2LR0ZqwMV1g",
        "colab_type": "text"
      },
      "source": [
        "### Creación de los datasets\n",
        "\n",
        "Similar al práctico anterior, tendremos datos que serán \"one-hot-encoded\", otros serán \"embeddings\" y otros serán numéricos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y39g7vGoMV1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It's important to always use the same one-hot length\n",
        "one_hot_columns = {\n",
        "    one_hot_col: dataset[one_hot_col].max()\n",
        "    for one_hot_col in ['Gender', 'Color1','Color2', 'FurLength']\n",
        "}\n",
        "embedded_columns = {\n",
        "    embedded_col: dataset[embedded_col].max() + 1\n",
        "    for embedded_col in ['Breed1', 'Breed2']\n",
        "}\n",
        "numeric_columns = ['Quantity','Age', 'Fee']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFAo7NVpMV1m",
        "colab_type": "text"
      },
      "source": [
        "## Generador del conjunto de datos\n",
        "\n",
        "Dada la naturaleza de los datos de texto, y que estos representan una secuencia de datos (que se da luego a una red recurrente o convolucional), en este caso no crearemos los datasets de antemano, sino que los generaremos a medida que el algoritmo de entrenamiento los pida. \n",
        "\n",
        "En particular, es porque las secuencias de texto pueden no tener el mismo tamaño (las oraciones tienen diferente cantidad de palabras), pero para que los modelos de redes las acepten, necesitamos rellenarlas (*padding*) de manera que todas tengan el mismo tamaño.\n",
        "\n",
        "En este paso también vamos a truncar aquellas secuencias de descripciones con más de `MAX_SEQUENCE_LEN` palabras, de manera que al hacer uso de `padded_batch` no lance un error al encontrarse con secuencias de tamaño mayor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqAftyxLMV1n",
        "colab_type": "code",
        "outputId": "bd47a411-3cba-4313-e187-d06645f46e72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "def dataset_generator(ds, test_data=False):\n",
        "    for _, row in ds.iterrows():\n",
        "        instance = {}\n",
        "        \n",
        "        # One hot encoded features\n",
        "        instance[\"direct_features\"] = np.hstack([\n",
        "            tf.keras.utils.to_categorical(row[one_hot_col] - 1, max_value)\n",
        "            for one_hot_col, max_value in one_hot_columns.items()\n",
        "        ])\n",
        "\n",
        "        # Numeric features (should be normalized beforehand)\n",
        "        # TODO: Add numeric features for row           \n",
        "        # ya trabajado en el práctico 1 aquí no se lo agrega\n",
        "        \n",
        "        # Embedded features\n",
        "        for embedded_col in embedded_columns:\n",
        "            instance[embedded_col] = [row[embedded_col]]\n",
        "        \n",
        "        # Document to indices for text data, truncated at MAX_SEQUENCE_LEN words\n",
        "        instance[\"description\"] = vocabulary.doc2idx(\n",
        "            row[\"TokenizedDescription\"],\n",
        "            unknown_word_index=len(vocabulary)\n",
        "        )[:MAX_SEQUENCE_LEN]\n",
        "        \n",
        "        # One hot encoded target for categorical crossentropy\n",
        "        if not test_data:\n",
        "            target = tf.keras.utils.to_categorical(row[target_col], nlabels)\n",
        "            yield instance, target\n",
        "        else:\n",
        "            yield instance\n",
        "\n",
        "# Set output types of the generator (for numeric types check the type is valid)\n",
        "instance_types = {\n",
        "    \"direct_features\": tf.float32,\n",
        "    \"description\": tf.int32\n",
        "}\n",
        "\n",
        "for embedded_col in embedded_columns:\n",
        "    instance_types[embedded_col] = tf.int32\n",
        "        \n",
        "tf_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: dataset_generator(dataset),\n",
        "    output_types=(instance_types, tf.int32)\n",
        ")\n",
        "\n",
        "for data, target in tf_dataset.take(2):\n",
        "    pprint(data)\n",
        "    pprint(target)\n",
        "    print()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Breed1': <tf.Tensor: id=39, shape=(1,), dtype=int32, numpy=array([299], dtype=int32)>,\n",
            " 'Breed2': <tf.Tensor: id=40, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
            " 'description': <tf.Tensor: id=41, shape=(42,), dtype=int32, numpy=\n",
            "array([23,  2, 20, 24,  4, 10,  1, 11, 26,  1, 27,  9,  6, 21,  3,  8, 15,\n",
            "       22, 33,  7, 13, 30,  1, 29, 18, 17,  1, 12, 31, 14,  5,  6, 16,  1,\n",
            "       19, 28, 25, 32, 23,  0,  5,  1], dtype=int32)>,\n",
            " 'direct_features': <tf.Tensor: id=42, shape=(20,), dtype=float32, numpy=\n",
            "array([1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "       1., 0., 0.], dtype=float32)>}\n",
            "<tf.Tensor: id=43, shape=(5,), dtype=int32, numpy=array([0, 0, 1, 0, 0], dtype=int32)>\n",
            "\n",
            "{'Breed1': <tf.Tensor: id=44, shape=(1,), dtype=int32, numpy=array([307], dtype=int32)>,\n",
            " 'Breed2': <tf.Tensor: id=45, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
            " 'description': <tf.Tensor: id=46, shape=(24,), dtype=int32, numpy=\n",
            "array([41, 42, 40, 35, 37, 35, 36, 35, 45, 50, 41, 44, 35, 46, 38, 48, 39,\n",
            "       47, 15, 43, 35, 49, 34, 34], dtype=int32)>,\n",
            " 'direct_features': <tf.Tensor: id=47, shape=(20,), dtype=float32, numpy=\n",
            "array([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "       1., 0., 0.], dtype=float32)>}\n",
            "<tf.Tensor: id=48, shape=(5,), dtype=int32, numpy=array([0, 0, 1, 0, 0], dtype=int32)>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCxQYBu_MV1s",
        "colab_type": "text"
      },
      "source": [
        "## Datos de entrenamiento y validación\n",
        "\n",
        "Ya generado el conjunto de datos base, tenemos que dividirlo en entrenamiento y validación. Además, como vamos a utilizar algunos datos que forman secuencias, los lotes (*batches*) de datos deben estar \"rellenados\" (*padded_batch*). \n",
        "\n",
        "Si bien rellenaremos \"todos\" los atributos, en la práctica el único que efectivamente se rellenará es el de *description* pues es el único con tamaños distintos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNygqBJ-MV1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_SIZE = int(dataset.shape[0] * 0.8)\n",
        "DEV_SIZE = dataset.shape[0] - TRAIN_SIZE\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "shuffled_dataset = tf_dataset.shuffle(TRAIN_SIZE + DEV_SIZE, seed=42)\n",
        "\n",
        "# Pad the datasets to the max value for all the \"non sequence\" features\n",
        "padding_shapes = (\n",
        "    {k: [-1] for k in [\"direct_features\"] + list(embedded_columns.keys())},\n",
        "    [-1]\n",
        ")\n",
        "\n",
        "# Pad to MAX_SEQUENCE_LEN for sequence features\n",
        "padding_shapes[0][\"description\"] = [MAX_SEQUENCE_LEN]\n",
        "\n",
        "# Pad values are irrelevant for non padded data\n",
        "padding_values = (\n",
        "    {k: 0 for k in list(embedded_columns.keys())},\n",
        "    0\n",
        ")\n",
        "\n",
        "# Padding value for direct features should be a float\n",
        "padding_values[0][\"direct_features\"] = np.float32(0)\n",
        "\n",
        "# Padding value for sequential features is the vocabulary length + 1\n",
        "padding_values[0][\"description\"] = len(vocabulary) + 1\n",
        "\n",
        "train_dataset = shuffled_dataset.skip(DEV_SIZE)\\\n",
        "    .padded_batch(BATCH_SIZE, padded_shapes=padding_shapes, padding_values=padding_values)\n",
        "\n",
        "dev_dataset = shuffled_dataset.take(DEV_SIZE)\\\n",
        "    .padded_batch(BATCH_SIZE, padded_shapes=padding_shapes, padding_values=padding_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3hGNJnBMV1x",
        "colab_type": "text"
      },
      "source": [
        "## Construyendo el modelo\n",
        "\n",
        "La diferencia fundamental de esta sección con lo trabajado en el práctico 1 es que aquí se incorpora el embedding de palabras.\n",
        "\n",
        "### Matriz de embeddings de palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAze30cfMV1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDINGS_DIM = 100  # Given by the model (in this case glove.6B.100d)\n",
        "\n",
        "embedding_matrix = np.zeros((len(vocabulary) + 2, 100))\n",
        "\n",
        "for widx, word in vocabulary.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[widx] = embedding_vector\n",
        "    else:\n",
        "        # Random normal initialization for words without embeddings\n",
        "        embedding_matrix[widx] = np.random.normal(size=(100,))  \n",
        "\n",
        "# Random normal initialization for unknown words\n",
        "embedding_matrix[len(vocabulary)] = np.random.normal(size=(100,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At7BPi-IMV13",
        "colab_type": "text"
      },
      "source": [
        "### Definiendo los inputs del modelo\n",
        "\n",
        "Definamos los inputs del modelo, con el agregado de la capa de embeddings de palabras inicializada en `embedding_matrix`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELScSrk8MV14",
        "colab_type": "code",
        "outputId": "2593298a-2164-48dc-e26e-1303a897871c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Add one input and one embedding for each embedded column\n",
        "embedding_layers = []\n",
        "inputs = []\n",
        "for embedded_col, max_value in embedded_columns.items():\n",
        "    input_layer = tf.keras.layers.Input(shape=(1,), name=embedded_col)\n",
        "    inputs.append(input_layer)\n",
        "    # Define the embedding layer\n",
        "    embedding_size = int(max_value / 4)\n",
        "    embedding_layers.append(\n",
        "        tf.squeeze(\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=max_value, \n",
        "                output_dim=embedding_size\n",
        "            )(input_layer), \n",
        "            axis=-2\n",
        "        )\n",
        "    )\n",
        "    print('Adding embedding of size {} for layer {}'.format(embedding_size, embedded_col))\n",
        "\n",
        "# Add the direct features already calculated\n",
        "direct_features_input = tf.keras.layers.Input(\n",
        "    shape=(sum(one_hot_columns.values()),), \n",
        "    name='direct_features'\n",
        ")\n",
        "inputs.append(direct_features_input)\n",
        "\n",
        "# Word embedding layer\n",
        "description_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LEN,), name=\"description\")\n",
        "inputs.append(description_input)\n",
        "\n",
        "word_embeddings_layer = tf.keras.layers.Embedding(\n",
        "    embedding_matrix.shape[0],\n",
        "    EMBEDDINGS_DIM,\n",
        "    weights=[embedding_matrix],\n",
        "    input_length=MAX_SEQUENCE_LEN,\n",
        "    trainable=False,\n",
        "    name=\"word_embedding\"\n",
        ")(description_input)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding embedding of size 77 for layer Breed1\n",
            "Adding embedding of size 77 for layer Breed2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4XSaGOUMV19",
        "colab_type": "text"
      },
      "source": [
        "### Definiendo la red que trabajará con el texto\n",
        "\n",
        "Antes de generar el *feature map* final entre los inputs y las clases, tenemos que generar el *feature map* de las secuencias de texto. \n",
        "\n",
        "Para ello pueden utilizar una red neuronal recurrente o convolucional.\n",
        "\n",
        "Pueden pensar dicha red como un submodelo del modelo general que se encarga de generar los atributos que representan la descripción de la mascota (recordemos que las redes se utilizan para hacer aprendizaje de representaciones).\n",
        "\n",
        "La red puede ser tan compleja como ustedes lo consideren pertinente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_geyYzuhMV1-",
        "colab_type": "code",
        "outputId": "89fd9ed7-5da0-422b-91ab-4c61f7e16f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "## TODO: Create a NN (CNN or RNN) for the description input (replace the next) \n",
        "#DONE!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "FILTER_WIDTHS = [2, 3, 5, 10]  # Take 2, 3, 5 & 10 words\n",
        "FILTER_COUNT = 64\n",
        "\n",
        "sequence_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LEN,), dtype='int64', name=\"input\")\n",
        "\n",
        "# The embedding layer is initialized with our embedding_matrix and is not trainable\n",
        "embeddings_layer = tf.keras.layers.Embedding(\n",
        "    embedding_matrix.shape[0],\n",
        "    EMBEDDINGS_DIM,\n",
        "    weights=[embedding_matrix],\n",
        "    input_length=MAX_SEQUENCE_LEN,\n",
        "    trainable=False,\n",
        "    name=\"word_embedding\"\n",
        ")\n",
        "\n",
        "embedded_sequences = embeddings_layer(sequence_input)\n",
        "\n",
        "conv_layers = []\n",
        "for filter_width in FILTER_WIDTHS:\n",
        "    layer = tf.keras.layers.Conv1D(\n",
        "        FILTER_COUNT,\n",
        "        filter_width,\n",
        "        activation=\"relu\",\n",
        "        name=\"conv_{}_words\".format(filter_width)\n",
        "    )(embedded_sequences)\n",
        "    layer = tf.keras.layers.GlobalMaxPooling1D(name=\"max_pool_{}_words\".format(filter_width))(layer)\n",
        "    conv_layers.append(layer)\n",
        "\n",
        "convolved_features = tf.keras.layers.Concatenate(name=\"convolved_features\")(conv_layers)\n",
        "output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")(convolved_features)\n",
        "model = tf.keras.models.Model(inputs=[sequence_input], outputs=[output], name=\"Red convolucional para texto\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"nadam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#DESCRIPTION_FEATURES_LAYER_SIZE = 256\n",
        "#DESCRIPTION_FEATURES_LAYER_SIZE = 512\n",
        "DESCRIPTION_FEATURES_LAYER_SIZE = 1024\n",
        "\n",
        "description_features = tf.keras.layers.Flatten()(word_embeddings_layer)  # This is a simple concatenation\n",
        "description_features = tf.keras.layers.Dense(units=DESCRIPTION_FEATURES_LAYER_SIZE, \n",
        "                                             activation=\"relu\", name=\"description_features\")(description_features)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Red convolucional para texto\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 55)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_embedding (Embedding)      (None, 55, 100)      1000200     input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_2_words (Conv1D)           (None, 54, 64)       12864       word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_3_words (Conv1D)           (None, 53, 64)       19264       word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_5_words (Conv1D)           (None, 51, 64)       32064       word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_10_words (Conv1D)          (None, 46, 64)       64064       word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_2_words (GlobalMaxPool (None, 64)           0           conv_2_words[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_3_words (GlobalMaxPool (None, 64)           0           conv_3_words[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_5_words (GlobalMaxPool (None, 64)           0           conv_5_words[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_10_words (GlobalMaxPoo (None, 64)           0           conv_10_words[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "convolved_features (Concatenate (None, 256)          0           max_pool_2_words[0][0]           \n",
            "                                                                 max_pool_3_words[0][0]           \n",
            "                                                                 max_pool_5_words[0][0]           \n",
            "                                                                 max_pool_10_words[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 1)            257         convolved_features[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 1,128,713\n",
            "Trainable params: 128,513\n",
            "Non-trainable params: 1,000,200\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLTDN8uLMV2M",
        "colab_type": "text"
      },
      "source": [
        "### Definiendo el *feature map* final de la red\n",
        "\n",
        "Ahora que tenemos nuestra representación de las descripciones, pasamos a combinarlo con los demás features en la última parte de nuestra red. Solo se muestra el modelo seleccionado luego de realizar varias pruebas.Se introducen varios dropout para reducir el sobreajuste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUhl3XkrMV2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#HIDDEN_LAYER_SIZE = 128\n",
        "#HIDDEN_LAYER_SIZE = 256\n",
        "HIDDEN_LAYER_SIZE = 512\n",
        "\n",
        "#DROPOUT_RATE = 0.5\n",
        "DROPOUT_RATE = 0.75\n",
        "\n",
        "\n",
        "feature_map = tf.keras.layers.Concatenate(name=\"feature_map\")(\n",
        "    embedding_layers + [description_features, direct_features_input]\n",
        ")\n",
        "\n",
        "dense1 = layers.Dense(HIDDEN_LAYER_SIZE, activation='relu')(feature_map)\n",
        "\n",
        "drop1 = layers.Dropout(DROPOUT_RATE)(dense1)\n",
        "\n",
        "dense2 = layers.Dense(HIDDEN_LAYER_SIZE / 2, activation='relu')(drop1)\n",
        "\n",
        "drop2 = layers.Dropout(DROPOUT_RATE)(dense2)\n",
        "\n",
        "dense3 = layers.Dense(HIDDEN_LAYER_SIZE / 4, activation='relu')(drop2)\n",
        "\n",
        "output_layer = layers.Dense(nlabels, activation='softmax')(dense3)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=output_layer, name=\"modelo_buenazo\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKLd5ZlnMV2T",
        "colab_type": "text"
      },
      "source": [
        "### Compilando y visualizando el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmxOKH3fMV2U",
        "colab_type": "code",
        "outputId": "d32e0a2f-65de-4a69-dc9f-9e51eb40d13f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='nadam',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"modelo_buenazo\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "description (InputLayer)        [(None, 55)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Breed1 (InputLayer)             [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Breed2 (InputLayer)             [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_embedding (Embedding)      (None, 55, 100)      1000200     description[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 77)        23716       Breed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1, 77)        23716       Breed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 5500)         0           word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze (TensorFlow [(None, 77)]         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze_1 (TensorFl [(None, 77)]         0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "description_features (Dense)    (None, 1024)         5633024     flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "direct_features (InputLayer)    [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "feature_map (Concatenate)       (None, 1198)         0           tf_op_layer_Squeeze[0][0]        \n",
            "                                                                 tf_op_layer_Squeeze_1[0][0]      \n",
            "                                                                 description_features[0][0]       \n",
            "                                                                 direct_features[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_96 (Dense)                (None, 512)          613888      feature_map[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 512)          0           dense_96[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_97 (Dense)                (None, 256)          131328      dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 256)          0           dense_97[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_98 (Dense)                (None, 128)          32896       dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_99 (Dense)                (None, 5)            645         dense_98[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 7,459,413\n",
            "Trainable params: 6,459,213\n",
            "Non-trainable params: 1,000,200\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTkwb1NwMV2Z",
        "colab_type": "code",
        "outputId": "b8d99cf8-4451-4a42-f7b9-4be6b4e0c6e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        }
      },
      "source": [
        "SVG(tf.keras.utils.model_to_dot(model, dpi=60).create(prog='dot', format='svg'))"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"646pt\" viewBox=\"0.00 0.00 961.00 775.00\" width=\"801pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 771)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-771 957,-771 957,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139770588240472 -->\n<g class=\"node\" id=\"node1\">\n<title>139770588240472</title>\n<polygon fill=\"none\" points=\"21.5,-730.5 21.5,-766.5 172.5,-766.5 172.5,-730.5 21.5,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97\" y=\"-744.8\">description: InputLayer</text>\n</g>\n<!-- 139770587167544 -->\n<g class=\"node\" id=\"node4\">\n<title>139770587167544</title>\n<polygon fill=\"none\" points=\"0,-657.5 0,-693.5 194,-693.5 194,-657.5 0,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97\" y=\"-671.8\">word_embedding: Embedding</text>\n</g>\n<!-- 139770588240472&#45;&gt;139770587167544 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139770588240472-&gt;139770587167544</title>\n<path d=\"M97,-730.4551C97,-722.3828 97,-712.6764 97,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"100.5001,-703.5903 97,-693.5904 93.5001,-703.5904 100.5001,-703.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139770588291024 -->\n<g class=\"node\" id=\"node2\">\n<title>139770588291024</title>\n<polygon fill=\"none\" points=\"269.5,-657.5 269.5,-693.5 398.5,-693.5 398.5,-657.5 269.5,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334\" y=\"-671.8\">Breed1: InputLayer</text>\n</g>\n<!-- 139770970734208 -->\n<g class=\"node\" id=\"node5\">\n<title>139770970734208</title>\n<polygon fill=\"none\" points=\"256,-584.5 256,-620.5 412,-620.5 412,-584.5 256,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334\" y=\"-598.8\">embedding: Embedding</text>\n</g>\n<!-- 139770588291024&#45;&gt;139770970734208 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139770588291024-&gt;139770970734208</title>\n<path d=\"M334,-657.4551C334,-649.3828 334,-639.6764 334,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"337.5001,-630.5903 334,-620.5904 330.5001,-630.5904 337.5001,-630.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139770970743920 -->\n<g class=\"node\" id=\"node3\">\n<title>139770970743920</title>\n<polygon fill=\"none\" points=\"560.5,-657.5 560.5,-693.5 689.5,-693.5 689.5,-657.5 560.5,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"625\" y=\"-671.8\">Breed2: InputLayer</text>\n</g>\n<!-- 139770970733312 -->\n<g class=\"node\" id=\"node6\">\n<title>139770970733312</title>\n<polygon fill=\"none\" points=\"539.5,-584.5 539.5,-620.5 710.5,-620.5 710.5,-584.5 539.5,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"625\" y=\"-598.8\">embedding_1: Embedding</text>\n</g>\n<!-- 139770970743920&#45;&gt;139770970733312 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139770970743920-&gt;139770970733312</title>\n<path d=\"M625,-657.4551C625,-649.3828 625,-639.6764 625,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"628.5001,-630.5903 625,-620.5904 621.5001,-630.5904 628.5001,-630.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139768899502032 -->\n<g class=\"node\" id=\"node7\">\n<title>139768899502032</title>\n<polygon fill=\"none\" points=\"40.5,-584.5 40.5,-620.5 153.5,-620.5 153.5,-584.5 40.5,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97\" y=\"-598.8\">flatten_3: Flatten</text>\n</g>\n<!-- 139770587167544&#45;&gt;139768899502032 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139770587167544-&gt;139768899502032</title>\n<path d=\"M97,-657.4551C97,-649.3828 97,-639.6764 97,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"100.5001,-630.5903 97,-620.5904 93.5001,-630.5904 100.5001,-630.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139770970298072 -->\n<g class=\"node\" id=\"node8\">\n<title>139770970298072</title>\n<polygon fill=\"none\" points=\"201.5,-511.5 201.5,-547.5 466.5,-547.5 466.5,-511.5 201.5,-511.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334\" y=\"-525.8\">tf_op_layer_Squeeze: TensorFlowOpLayer</text>\n</g>\n<!-- 139770970734208&#45;&gt;139770970298072 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139770970734208-&gt;139770970298072</title>\n<path d=\"M334,-584.4551C334,-576.3828 334,-566.6764 334,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"337.5001,-557.5903 334,-547.5904 330.5001,-557.5904 337.5001,-557.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139770588290184 -->\n<g class=\"node\" id=\"node9\">\n<title>139770588290184</title>\n<polygon fill=\"none\" points=\"485,-511.5 485,-547.5 765,-547.5 765,-511.5 485,-511.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"625\" y=\"-525.8\">tf_op_layer_Squeeze_1: TensorFlowOpLayer</text>\n</g>\n<!-- 139770970733312&#45;&gt;139770588290184 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139770970733312-&gt;139770588290184</title>\n<path d=\"M625,-584.4551C625,-576.3828 625,-566.6764 625,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"628.5001,-557.5903 625,-547.5904 621.5001,-557.5904 628.5001,-557.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139768899501864 -->\n<g class=\"node\" id=\"node10\">\n<title>139768899501864</title>\n<polygon fill=\"none\" points=\"10.5,-511.5 10.5,-547.5 183.5,-547.5 183.5,-511.5 10.5,-511.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97\" y=\"-525.8\">description_features: Dense</text>\n</g>\n<!-- 139768899502032&#45;&gt;139768899501864 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139768899502032-&gt;139768899501864</title>\n<path d=\"M97,-584.4551C97,-576.3828 97,-566.6764 97,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"100.5001,-557.5903 97,-547.5904 93.5001,-557.5904 100.5001,-557.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139769128976168 -->\n<g class=\"node\" id=\"node12\">\n<title>139769128976168</title>\n<polygon fill=\"none\" points=\"397,-438.5 397,-474.5 561,-474.5 561,-438.5 397,-438.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479\" y=\"-452.8\">feature_map: Concatenate</text>\n</g>\n<!-- 139770970298072&#45;&gt;139769128976168 -->\n<g class=\"edge\" id=\"edge8\">\n<title>139770970298072-&gt;139769128976168</title>\n<path d=\"M369.8427,-511.4551C389.1879,-501.7157 413.2584,-489.5975 433.8698,-479.2207\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"435.709,-482.2133 443.0671,-474.5904 432.5613,-475.961 435.709,-482.2133\" stroke=\"#000000\"/>\n</g>\n<!-- 139770588290184&#45;&gt;139769128976168 -->\n<g class=\"edge\" id=\"edge9\">\n<title>139770588290184-&gt;139769128976168</title>\n<path d=\"M588.9101,-511.4551C569.3438,-501.6719 544.9765,-489.4883 524.1612,-479.0806\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"525.6903,-475.9321 515.1807,-474.5904 522.5597,-482.193 525.6903,-475.9321\" stroke=\"#000000\"/>\n</g>\n<!-- 139768899501864&#45;&gt;139769128976168 -->\n<g class=\"edge\" id=\"edge10\">\n<title>139768899501864-&gt;139769128976168</title>\n<path d=\"M183.7181,-512.5955C186.5111,-512.0565 189.2774,-511.5236 192,-511 257.0718,-498.4857 330.4796,-484.5492 386.7646,-473.9018\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"387.5368,-477.3179 396.7122,-472.0206 386.236,-470.4399 387.5368,-477.3179\" stroke=\"#000000\"/>\n</g>\n<!-- 139771125895688 -->\n<g class=\"node\" id=\"node11\">\n<title>139771125895688</title>\n<polygon fill=\"none\" points=\"783,-511.5 783,-547.5 953,-547.5 953,-511.5 783,-511.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"868\" y=\"-525.8\">direct_features: InputLayer</text>\n</g>\n<!-- 139771125895688&#45;&gt;139769128976168 -->\n<g class=\"edge\" id=\"edge11\">\n<title>139771125895688-&gt;139769128976168</title>\n<path d=\"M782.9542,-512.7162C779.9328,-512.1346 776.9408,-511.561 774,-511 706.052,-498.0375 629.2042,-483.8556 570.9807,-473.2087\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"571.5344,-469.752 561.0681,-471.3973 570.276,-476.638 571.5344,-469.752\" stroke=\"#000000\"/>\n</g>\n<!-- 139769128976112 -->\n<g class=\"node\" id=\"node13\">\n<title>139769128976112</title>\n<polygon fill=\"none\" points=\"422,-365.5 422,-401.5 536,-401.5 536,-365.5 422,-365.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479\" y=\"-379.8\">dense_96: Dense</text>\n</g>\n<!-- 139769128976168&#45;&gt;139769128976112 -->\n<g class=\"edge\" id=\"edge12\">\n<title>139769128976168-&gt;139769128976112</title>\n<path d=\"M479,-438.4551C479,-430.3828 479,-420.6764 479,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"482.5001,-411.5903 479,-401.5904 475.5001,-411.5904 482.5001,-411.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139769129177776 -->\n<g class=\"node\" id=\"node14\">\n<title>139769129177776</title>\n<polygon fill=\"none\" points=\"408.5,-292.5 408.5,-328.5 549.5,-328.5 549.5,-292.5 408.5,-292.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479\" y=\"-306.8\">dropout_48: Dropout</text>\n</g>\n<!-- 139769128976112&#45;&gt;139769129177776 -->\n<g class=\"edge\" id=\"edge13\">\n<title>139769128976112-&gt;139769129177776</title>\n<path d=\"M479,-365.4551C479,-357.3828 479,-347.6764 479,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"482.5001,-338.5903 479,-328.5904 475.5001,-338.5904 482.5001,-338.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139769129179624 -->\n<g class=\"node\" id=\"node15\">\n<title>139769129179624</title>\n<polygon fill=\"none\" points=\"422,-219.5 422,-255.5 536,-255.5 536,-219.5 422,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479\" y=\"-233.8\">dense_97: Dense</text>\n</g>\n<!-- 139769129177776&#45;&gt;139769129179624 -->\n<g class=\"edge\" id=\"edge14\">\n<title>139769129177776-&gt;139769129179624</title>\n<path d=\"M479,-292.4551C479,-284.3828 479,-274.6764 479,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"482.5001,-265.5903 479,-255.5904 475.5001,-265.5904 482.5001,-265.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139768837289912 -->\n<g class=\"node\" id=\"node16\">\n<title>139768837289912</title>\n<polygon fill=\"none\" points=\"408.5,-146.5 408.5,-182.5 549.5,-182.5 549.5,-146.5 408.5,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479\" y=\"-160.8\">dropout_49: Dropout</text>\n</g>\n<!-- 139769129179624&#45;&gt;139768837289912 -->\n<g class=\"edge\" id=\"edge15\">\n<title>139769129179624-&gt;139768837289912</title>\n<path d=\"M479,-219.4551C479,-211.3828 479,-201.6764 479,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"482.5001,-192.5903 479,-182.5904 475.5001,-192.5904 482.5001,-192.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139769129111000 -->\n<g class=\"node\" id=\"node17\">\n<title>139769129111000</title>\n<polygon fill=\"none\" points=\"422,-73.5 422,-109.5 536,-109.5 536,-73.5 422,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479\" y=\"-87.8\">dense_98: Dense</text>\n</g>\n<!-- 139768837289912&#45;&gt;139769129111000 -->\n<g class=\"edge\" id=\"edge16\">\n<title>139768837289912-&gt;139769129111000</title>\n<path d=\"M479,-146.4551C479,-138.3828 479,-128.6764 479,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"482.5001,-119.5903 479,-109.5904 475.5001,-119.5904 482.5001,-119.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139768882565072 -->\n<g class=\"node\" id=\"node18\">\n<title>139768882565072</title>\n<polygon fill=\"none\" points=\"422,-.5 422,-36.5 536,-36.5 536,-.5 422,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479\" y=\"-14.8\">dense_99: Dense</text>\n</g>\n<!-- 139769129111000&#45;&gt;139768882565072 -->\n<g class=\"edge\" id=\"edge17\">\n<title>139769129111000-&gt;139768882565072</title>\n<path d=\"M479,-73.4551C479,-65.3828 479,-55.6764 479,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"482.5001,-46.5903 479,-36.5904 475.5001,-46.5904 482.5001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKDufjFvMV2d",
        "colab_type": "text"
      },
      "source": [
        "## Entrenando el modelo\n",
        "\n",
        "Para entrenar el modelo es igual al caso anterior, ya generados el conjunto de datos correspondiente. Lo entrenamos con ayuda de `mlflow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NLYXXDcMV2f",
        "colab_type": "code",
        "outputId": "1e54e91c-bc31-4672-b998-ba6ad1e9c912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "import mlflow\n",
        "import mlflow.keras\n",
        "\n",
        "mlflow.set_experiment('modelo_buenazo')\n",
        "\n",
        "with mlflow.start_run(nested=True):\n",
        "    # Log model hiperparameters first\n",
        "    mlflow.log_param('description_features_layer_size', DESCRIPTION_FEATURES_LAYER_SIZE)\n",
        "    mlflow.log_param('hidden_layer_size', HIDDEN_LAYER_SIZE)\n",
        "    mlflow.log_param('embedded_columns', embedded_columns)\n",
        "    mlflow.log_param('one_hot_columns', one_hot_columns)\n",
        "    # mlflow.log_param('numerical_columns', numerical_columns)  # Not using these yet\n",
        "    \n",
        "    # Train\n",
        "    epochs = 10\n",
        "    history = model.fit(train_dataset, epochs=epochs)\n",
        "    \n",
        "    # Evaluate\n",
        "    loss, accuracy = model.evaluate(dev_dataset, verbose=0)\n",
        "    print(\"\\n*** Validation loss: {} - accuracy: {}\".format(loss, accuracy))\n",
        "    mlflow.log_metric('epochs', epochs)\n",
        "    mlflow.log_metric('train_loss', history.history[\"loss\"][-1])\n",
        "    mlflow.log_metric('train_accuracy', history.history[\"accuracy\"][-1])\n",
        "    mlflow.log_metric('validation_loss', loss)\n",
        "    mlflow.log_metric('validation_accuracy', accuracy)\n",
        "\n",
        "    model1 =mlflow.log_metric('validation_loss', loss)\n",
        "    mlflow.log_metric('validation_accuracy', accuracy)\n",
        "    mlflow.keras.log_model(model, 'models')"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "67/67 [==============================] - 8s 123ms/step - loss: 1.4533 - accuracy: 0.4430\n",
            "Epoch 2/10\n",
            "67/67 [==============================] - 7s 103ms/step - loss: 0.5730 - accuracy: 0.8117\n",
            "Epoch 3/10\n",
            "67/67 [==============================] - 6s 96ms/step - loss: 0.3037 - accuracy: 0.9062\n",
            "Epoch 4/10\n",
            "67/67 [==============================] - 7s 97ms/step - loss: 0.2419 - accuracy: 0.9320\n",
            "Epoch 5/10\n",
            "67/67 [==============================] - 7s 98ms/step - loss: 0.1715 - accuracy: 0.9460\n",
            "Epoch 6/10\n",
            "67/67 [==============================] - 6s 96ms/step - loss: 0.1746 - accuracy: 0.9523\n",
            "Epoch 7/10\n",
            "67/67 [==============================] - 6s 97ms/step - loss: 0.1428 - accuracy: 0.9556\n",
            "Epoch 8/10\n",
            "67/67 [==============================] - 6s 97ms/step - loss: 0.1360 - accuracy: 0.9605\n",
            "Epoch 9/10\n",
            "67/67 [==============================] - 7s 97ms/step - loss: 0.1305 - accuracy: 0.9610\n",
            "Epoch 10/10\n",
            "67/67 [==============================] - 7s 98ms/step - loss: 0.1262 - accuracy: 0.9626\n",
            "\n",
            "*** Validation loss: 0.07573777491993763 - accuracy: 0.9707132577896118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBXHSePJMV2m",
        "colab_type": "text"
      },
      "source": [
        "Es intrigante notar que sobreajusta bastante (en test accuracy= 0.38) aún cuando se introdujeron varios dropouts y se está usando una probabilidad de dropeo relativamente elevada (0.50). Como conocemos de antemano este dataset, sabemos que es difícil obtener un accuracy superior al 40% en el conjunto de datos de test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEwkMQxLMV2n",
        "colab_type": "text"
      },
      "source": [
        "## Evaluando el modelo sobre los datos de evaluación para la competencia\n",
        "\n",
        "Comenzamos cargando el conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qikoGesbMV2o",
        "colab_type": "code",
        "outputId": "e2898a16-0217-4332-c4c9-7ad9d5a5bb9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "test_dataset = pd.read_csv(os.path.join(DATA_DIRECTORY, 'test.csv'))\n",
        "test_dataset.head()"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Age</th>\n",
              "      <th>Breed1</th>\n",
              "      <th>Breed2</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Color1</th>\n",
              "      <th>Color2</th>\n",
              "      <th>Color3</th>\n",
              "      <th>MaturitySize</th>\n",
              "      <th>FurLength</th>\n",
              "      <th>Vaccinated</th>\n",
              "      <th>Dewormed</th>\n",
              "      <th>Sterilized</th>\n",
              "      <th>Health</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Fee</th>\n",
              "      <th>State</th>\n",
              "      <th>Description</th>\n",
              "      <th>PID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>265</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>41401</td>\n",
              "      <td>I just found it alone yesterday near my apartm...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>307</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>41326</td>\n",
              "      <td>Their pregnant mother was dumped by her irresp...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>307</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>41326</td>\n",
              "      <td>Siu Pak just give birth on 13/6/10 to 6puppies...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>265</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>41326</td>\n",
              "      <td>Very manja and gentle stray cat found, we woul...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>264</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>41326</td>\n",
              "      <td>Kali is a super playful kitten who is on the g...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Type  Age  ...                                        Description  PID\n",
              "0     2    1  ...  I just found it alone yesterday near my apartm...    1\n",
              "1     1    1  ...  Their pregnant mother was dumped by her irresp...    2\n",
              "2     1    0  ...  Siu Pak just give birth on 13/6/10 to 6puppies...    7\n",
              "3     2   12  ...  Very manja and gentle stray cat found, we woul...    9\n",
              "4     2    3  ...  Kali is a super playful kitten who is on the g...   11\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D23UhBSXMV2s",
        "colab_type": "text"
      },
      "source": [
        "## Creamos el conjunto de datos para darle al modelo entrenado\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI5FPJp0MV2u",
        "colab_type": "code",
        "outputId": "2a26df40-017f-4092-b09e-cf009db4c708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# First tokenize the description\n",
        "\n",
        "test_dataset[\"TokenizedDescription\"] = test_dataset[\"Description\"]\\\n",
        "    .fillna(value=\"\").apply(tokenize_description)\n",
        "\n",
        "# Generate the basic TF dataset\n",
        "\n",
        "tf_test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: dataset_generator(test_dataset, True),\n",
        "    output_types=instance_types  # It should have the same instance types\n",
        ")\n",
        "\n",
        "for data in tf_test_dataset.take(2):  # The dataset only returns a data instance now (no target)\n",
        "    pprint(data)\n",
        "    print()"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Breed1': <tf.Tensor: id=89676, shape=(1,), dtype=int32, numpy=array([265], dtype=int32)>,\n",
            " 'Breed2': <tf.Tensor: id=89677, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
            " 'description': <tf.Tensor: id=89678, shape=(13,), dtype=int32, numpy=\n",
            "array([ 116,  429, 1371,  991,  189,    1, 7873, 1043,   62,  600,  728,\n",
            "          5,    1], dtype=int32)>,\n",
            " 'direct_features': <tf.Tensor: id=89679, shape=(20,), dtype=float32, numpy=\n",
            "array([1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "       0., 1., 0.], dtype=float32)>}\n",
            "\n",
            "{'Breed1': <tf.Tensor: id=89680, shape=(1,), dtype=int32, numpy=array([307], dtype=int32)>,\n",
            " 'Breed2': <tf.Tensor: id=89681, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
            " 'description': <tf.Tensor: id=89682, shape=(47,), dtype=int32, numpy=\n",
            "array([ 945,  154,  256, 2049,  105,  403,  991, 4677,  552,  545,    1,\n",
            "        142,  134,  403,    1,  118,  210,   73,    1,  533,  387,   35,\n",
            "        394,  272,   98,   62,    1,  464,  411,  151, 1401,   42,  253,\n",
            "          1,  825,   35, 4659,  247, 4155, 1402, 1403,    1,   43,   52,\n",
            "        599,   38,    1], dtype=int32)>,\n",
            " 'direct_features': <tf.Tensor: id=89683, shape=(20,), dtype=float32, numpy=\n",
            "array([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "       0., 1., 0.], dtype=float32)>}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9knEAv_MV2y",
        "colab_type": "text"
      },
      "source": [
        "## Padding batches\n",
        "\n",
        "Por último, y previo a probar el modelo sobre los datos de evaluación, generamos el conjunto de datos \"rellenado\". \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6plVOErhMV2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = tf_test_dataset.padded_batch(\n",
        "    BATCH_SIZE, \n",
        "    padded_shapes=padding_shapes[0], \n",
        "    padding_values=padding_values[0]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f81GR9kMV24",
        "colab_type": "text"
      },
      "source": [
        "## Correr el modelo\n",
        "\n",
        "El último paso es correr el modelo sobre los datos de evaluación para conseguir las predicciones a enviar a la competencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucI-lg9kMV26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset[\"AdoptionSpeed\"] = model.predict(test_data).argmax(axis=1)\n",
        "\n",
        "test_dataset.to_csv(\"./submission.csv\", index=False, columns=[\"PID\", \"AdoptionSpeed\"])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}